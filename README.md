# Synthetic Dataset Mask R-CNN with RGB-D Fusion for unknown objects segmentation
The project thesis: https://drive.google.com/file/d/1pGWtNtZCUU8L-wxvL3iz4RDKf33T3_we/view?usp=drive_link
## Environment install
```bash
$ conda create -n <xxx> python=3.7
$ conda activate <xxx>
$ pip install torch torchvision
$ pip install imgviz tqdm tensorboardX pandas opencv-python imutils pyfastnoisesimd scikit-image pycocotools
$ pip install pyrealsense2 # for realsense d435 demo
$ conda activate <xxx>
```
If you use the [zed_demo](https://github.com/Pengfeicc/SF-Mask-RCNN/blob/main/zed_demo.py).py, you must install the zed-python-api first: https://github.com/stereolabs/zed-python-api.git

## Download the pre-weight
In the thesis, we use [confidence fusion](https://drive.google.com/file/d/1uuGsZBEkUtP6jcyXcGGEF-3PCmIy7msv/view?usp=drive_link) weight to train on our synthetic dataset, and put this weight in the root catelog of this model.

## Download the synthetic dataset for training
1. This dataset provide 5000 rgb images, 5000 depth images, 5000 modal masks and depths values .npy. you can download [here](https://drive.google.com/file/d/1zP2UUxzW6rXKw0WwhuMYu3O2Ki5jA-th/view?usp=drive_link) (>23GB)
2. Also you can download another well-strutured dataset-[WISDOM](https://drive.google.com/file/d/1nlm7MqlbOrMkbz6JHjJ7R03xdbQRRiT2/view?usp=drive_link) for training.
3. Set the path to the dataset and pretrained weights (You can put this into your bash profile)
```bash
$ export WISDOM_PATH={/path/to/the/synthetic/dataset}
$ export WEIGHT_PATH={/path/to/the/pretrained/weights}
```
### Create a custom synthetic dataset
in our thesis, this synthetic segmentation dataset is generated by [BlenderProc](https://github.com/DLR-RM/BlenderProc). The synthetic dataset generator is modified on the BOP challenge examples(https://bop.felk.cvut.cz/challenges/), which is published on this [site](https://github.com/Pengfeicc/BOP-Sampling.git).

## Train
1. Unzip the downloaded dataset, and modify the dataset_path of the config file correspondingly.
2. Run this command:
   ```bash
   $ python train.py --gpu 0 --cfg rgb_noisydepth_confidencefusion
   ```
3. To fine-tune the WISDOM dataset
   ```bash
   $ python train.py --gpu 0 --cfg rgb_noisydepth_confidencefusion_FT --resume
   ```
## Evaluation
To evaluate (confidence fusion, RGB-noisy depth as input) on a WISDOM dataset
```bash
$ python eval.py --gpu 0 --cfg rgb_noisydepth_confidencefusion \
    --eval_data wisdom \
    --dataset_path xxx xxx \
    --weight_path xxx xxx/ConfidenceFusion.tar 
```

## Updates
### Test in different scenes by realsense d435(2022/05/27)
![similar to pretrained mode](./imgs/1.gif)
![similar to petrained model and one different](./imgs/2.gif)
![similar to pretrained model and two different_different pose](./imgs/3.gif)
![all kinds of different unknowns](./imgs/4.gif)
![all kinds of objs with a small overlap](./imgs/5.gif)
![all kinds of objs with a heavy overlap](./imgs/6.gif)

### Integrate ZED Camera to inference (2022/06/10)
> [zed_demo.py](https://github.com/Pengfeicc/SF-Mask-RCNN/blob/main/zed_demo.py)

### (TEST ONLY) Re-generate synthetic dataset and retrain for better result? (2022/06/..)
